{
  "paragraphs": [
    {
      "text": "%md\n# Big Data: Spark RDDs (exercises)\nObjectives:\n* Show the basic understanding of (creating) RDDs\n* Get to know what happens with partitions and partitioners in Spark\n* Work with the Shakespeare Data",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h1>Big Data: Spark RDDs (exercises)</h1>\n<p>Objectives:</p>\n<ul>\n<li>Show the basic understanding of (creating) RDDs</li>\n<li>Get to know what happens with partitions and partitioners in Spark</li>\n<li>Work with the Shakespeare Data</li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142657_1978012539",
      "id": "20210323-214017_1217973860",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:345"
    },
    {
      "text": "%spark\n// Sanity check for Spark and Scala\nprintf(\"Spark %s\\nScala %s\", sc.version, util.Properties.versionString)",
      "user": "anonymous",
      "dateUpdated": "2023-03-18T20:48:47+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Spark 3.1.1\nScala version 2.12.10"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142657_1333989253",
      "id": "paragraph_1631794243863_294069186",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "dateStarted": "2023-03-18T20:48:48+0000",
      "dateFinished": "2023-03-18T20:49:26+0000",
      "status": "FINISHED",
      "$$hashKey": "object:346"
    },
    {
      "text": "%md\n## Exercise 1.1\nCheck the Spark RDD programming guide,\n\n* https://spark.apache.org/docs/3.1.1/rdd-programming-guide.html\n\nand answer the following questions (in 1-3 sentences):\n\na. What is an RDD?\nb. Explain the differences between transformations and actions.\nc. Give two example transformations where shuffling the data is needed.\n\na. RDD means Resilient Distributed Datasets, which are fundamental data structures of Apache Spark that allow for distributed data processing.\n\nb. Transformations are operations that create a new RDD from an existing one, whereas actions are operations that return a value or perform an action on an RDD. Transformations are lazy, meaning that they don't get executed until an action is called on the RDD.\n\nc. Two examples of transformations where shuffling the data is needed are groupByKey() and reduceByKey(). The groupByKey() transformation groups the values of each key in an RDD together, which requires shuffling the data to move all values with the same key to the same machine.",
      "user": "anonymous",
      "dateUpdated": "2023-03-18T21:22:29+0000",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Exercise 1.1</h2>\n<p>Check the Spark RDD programming guide,</p>\n<ul>\n<li><a href=\"https://spark.apache.org/docs/3.1.1/rdd-programming-guide.html\">https://spark.apache.org/docs/3.1.1/rdd-programming-guide.html</a></li>\n</ul>\n<p>and answer the following questions (in 1-3 sentences):</p>\n<p>a. What is an RDD?<br />\nb. Explain the differences between transformations and actions.<br />\nc. Give two example transformations where shuffling the data is needed.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142657_1145134906",
      "id": "paragraph_1638911853147_1545900617",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:347"
    },
    {
      "text": "%md\n## Exercise 1.2\na. Give the line of code for creating an RDD using the `parallelize()` operation with the numbers 0 to 799 in it. We want this RDD to be split up in 8 different partitions.",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Exercise 1.2</h2>\n<p>a. Give the line of code for creating an RDD using the <code>parallelize()</code> operation with the numbers 0 to 799 in it. We want this RDD to be split up in 8 different partitions.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142657_1135302369",
      "id": "paragraph_1631794220150_1011381899",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:348"
    },
    {
      "text": "%spark\nval rdd = sc.parallelize(0 to 799, 8) //",
      "user": "anonymous",
      "dateUpdated": "2023-03-18T20:49:35+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrdd\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[Int]\u001b[0m = ParallelCollectionRDD[0] at parallelize at <console>:26\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_1001011854",
      "id": "paragraph_1631794251921_1962384498",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "dateStarted": "2023-03-18T20:49:36+0000",
      "dateFinished": "2023-03-18T20:49:37+0000",
      "status": "FINISHED",
      "$$hashKey": "object:349"
    },
    {
      "text": "%md\nb. If we execute the line of code from answer a, what will happen?",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>b. If we execute the line of code from answer a, what will happen?</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_479723706",
      "id": "paragraph_1644578073323_1894678196",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:350"
    },
    {
      "text": "%md\nWe now execute the following line of code:",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>We now execute the following line of code:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_143813688",
      "id": "paragraph_1631794274564_1276090416",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:351"
    },
    {
      "text": "%spark\nval sample = rdd.takeSample(false,8)",
      "user": "anonymous",
      "dateUpdated": "2023-03-18T20:49:41+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34msample\u001b[0m: \u001b[1m\u001b[32mArray[Int]\u001b[0m = Array(668, 184, 482, 208, 78, 356, 144, 168)\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://94a6d63f794b:4040/jobs/job?id=0",
              "$$hashKey": "object:4190"
            },
            {
              "jobUrl": "http://94a6d63f794b:4040/jobs/job?id=1",
              "$$hashKey": "object:4191"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_1304838774",
      "id": "paragraph_1631794284910_1878093945",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "dateStarted": "2023-03-18T20:49:41+0000",
      "dateFinished": "2023-03-18T20:49:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:352"
    },
    {
      "text": "%md\nc. What do the parameters ‘false’ and ‘8’ mean?\nd. After executing the code block above, look at the [stages](http://localhost:4040/stages/). Why did Spark fire off eight different tasks?\ne. (optional/advanced) Why would Spark create two jobs to take the sample? _Hint: read the [code on takeSample()](https://github.com/apache/spark/blob/1d550c4e90275ab418b9161925049239227f3dc9/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L620)._\n\nc. The first parameter 'false' is a Boolean flag indicating whether or not to sample with replacement. In this case, the flag is set to 'false' which means that the sampling will be done without replacement. The second parameter '8' specifies the number of elements to be sampled.\n\nd. After executing the code block, Spark fired off eight different tasks because the RDD is partitioned, and each task sampled a different partition. ",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T21:24:29+0000",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>c. What do the parameters ‘false’ and ‘8’ mean?<br />\nd. After executing the code block above, look at the <a href=\"http://localhost:4040/stages/\">stages</a>. Why did Spark fire off eight different tasks?<br />\ne. (optional/advanced) Why would Spark create two jobs to take the sample? <em>Hint: read the <a href=\"https://github.com/apache/spark/blob/1d550c4e90275ab418b9161925049239227f3dc9/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L620\">code on takeSample()</a>.</em></p>\n<p>c. The first parameter &lsquo;false&rsquo; is a Boolean flag indicating whether or not to sample with replacement. In this case, the flag is set to &lsquo;false&rsquo; which means that the sampling will be done without replacement. The second parameter &lsquo;8&rsquo; specifies the number of elements to be sampled.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_1313987426",
      "id": "paragraph_1631794296169_1420267951",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "dateStarted": "2023-03-16T21:23:29+0000",
      "dateFinished": "2023-03-16T21:23:32+0000",
      "status": "FINISHED",
      "$$hashKey": "object:353"
    },
    {
      "text": "%md \n## Exercise 1.3\n(In one or a couple of sentences:)\na. Describe what the functions `flatmap()` and `map()` do and its differences.\nb. Describe what the functions `take()` and `collect()` do and what can be hazardous in using `collect()`.\nc. Describe what `cache()` does and what can be hazardous about it.\n\na. The flatMap() and map() functions are used to transform the elements of an RDD in Spark using Scala. The map() function applies a function to each element of an RDD and returns a new RDD, while flatMap() applies a function that returns a sequence of values for each input element and then flattens the results into a new RDD.\n\nb. The take() function returns the first n elements of an RDD, while collect() returns all the elements of an RDD as an array to the driver program, which can be hazardous if the RDD contains a large number of elements, as it may cause memory issues on the driver program.\n\nc. The cache() function is used to persist an RDD in memory, which can speed up subsequent operations that use the same RDD. However, caching too many RDDs can lead to memory issues and slow down the overall performance of the Spark application. \n",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T21:27:07+0000",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Exercise 1.3</h2>\n<p>(In one or a couple of sentences:)<br />\na. Describe what the functions <code>flatmap()</code> and <code>map()</code> do and its differences.<br />\nb. Describe what the functions <code>take()</code> and <code>collect()</code> do and what can be hazardous in using <code>collect()</code>.<br />\nc. Describe what <code>cache()</code> does and what can be hazardous about it.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_1328848755",
      "id": "paragraph_1631794330892_289632298",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:354"
    },
    {
      "text": "%md\n## Exercise 2.1\na. For each value, write down the number of partitions, whether there is a partitioner assigned to and if so, which one. Also, briefly indicate how you got the answer.",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Exercise 2.1</h2>\n<p>a. For each value, write down the number of partitions, whether there is a partitioner assigned to and if so, which one. Also, briefly indicate how you got the answer.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_302303700",
      "id": "paragraph_1631197722812_724425735",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:355"
    },
    {
      "text": "%spark\nval A = sc.parallelize(0 to 999,8)\nA.take(20)",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_868837375",
      "id": "20210323-214017_1526376678",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:356"
    },
    {
      "text": "%md\nThe variable A is created with 8 partitions because it is passed as the second argument in the parallelize method. Since the range is from 0 to 999, there are 1000 elements in total. Therefore, each partition should have approximately 1000/8 = 125 elements. There is no specific partitioner assigned to A since it is not grouped or joined with any other RDD.\n",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T21:29:44+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679002174624_304712808",
      "id": "paragraph_1679002174624_304712808",
      "dateCreated": "2023-03-16T21:29:34+0000",
      "status": "READY",
      "$$hashKey": "object:357"
    },
    {
      "text": "%spark\nval B = A.map(x => (x % 100, 1000 - x))\nB.take(20)",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_1868156998",
      "id": "20210323-214017_753817829",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:358"
    },
    {
      "text": "%md\nThe variable B is created by applying a transformation to A. Since A has 8 partitions, applying the map transformation to each element of A should also result in 8 partitions in B. There is no specific partitioner assigned to B since it is not grouped with any other RDD.",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T21:32:47+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679002185062_87416794",
      "id": "paragraph_1679002185062_87416794",
      "dateCreated": "2023-03-16T21:29:45+0000",
      "status": "READY",
      "$$hashKey": "object:359"
    },
    {
      "text": "%spark\nval C = B.groupByKey()\nC.take(20)",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_975003223",
      "id": "20210323-214017_1628283214",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:360"
    },
    {
      "text": "%md\nThe variable C is created by grouping the elements in B by their key. Since B has 8 partitions, applying the groupByKey should also result in 8 partitions in C. The partitioner assigned to C is then groupByKey.",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T21:34:38+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679002205534_560706701",
      "id": "paragraph_1679002205534_560706701",
      "dateCreated": "2023-03-16T21:30:05+0000",
      "status": "READY",
      "$$hashKey": "object:361"
    },
    {
      "text": "%spark\nimport org.apache.spark.HashPartitioner\nval D = B.partitionBy(new HashPartitioner(2))\nD.take(20)",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_1939661176",
      "id": "20210323-214017_1339820103",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:362"
    },
    {
      "text": "%md\nD is created by applying the partitionBy transformation on B and its number of partitions is 2 since it's precised in the argument of HashPartitioner. The partitioner assigned to B is then HashPartitioner.\n",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T21:37:11+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679002222118_1446016652",
      "id": "paragraph_1679002222118_1446016652",
      "dateCreated": "2023-03-16T21:30:22+0000",
      "status": "READY",
      "$$hashKey": "object:363"
    },
    {
      "text": "%spark\nval E = D.groupByKey()\nE.take(20)",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_89210247",
      "id": "paragraph_1631198002576_655744250",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:364"
    },
    {
      "text": "%md\nE is created by applying the groupByKey transformation on D and it's number of partitions is 2 since D has 2 partitioner. The partitioner assigned to E is then HashPartitioner.",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T21:38:49+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679002631795_382450285",
      "id": "paragraph_1679002631795_382450285",
      "dateCreated": "2023-03-16T21:37:11+0000",
      "status": "READY",
      "$$hashKey": "object:365"
    },
    {
      "text": "%md\nb. Why does the `take()` of D retrieve different pairs than B?\nc. Explain why the keys are no longer in order in the `take()` of C and E.\n\nb. The take() of D retrieves different pairs than B because D has been partitioned using the HashPartitioner with the number of partitions set to 2, which means that the elements of B have been shuffled across the two partitions based on their keys. Therefore, the order of the elements in each partition is not guaranteed to be the same as in B. The take() method returns the first n elements of an RDD, but since the order of elements in D has changed due to the shuffling, the first n elements of D can be different from the first n elements of B.\n\nc. The keys are no longer in order in the take() of C and E because the groupByKey transformation does not guarantee the order of keys in the resulting RDD. When C and E are created, the values for each key are collected from all the partitions where they exist and are grouped together. Since this involves data shuffling, the order of keys within each partition can change.",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T21:39:38+0000",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>b. Why does the <code>take()</code> of D retrieve different pairs than B?<br />\nc. Explain why the keys are no longer in order in the <code>take()</code> of C and E.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142658_1896603852",
      "id": "paragraph_1638909440005_1799348093",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:366"
    },
    {
      "text": "%md\n## Exercise 2.2\nFor each value, write down the number of partitions, whether there is a partitioner assigned to and if so, which one. Also, briefly indicate how you got the answer.",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Exercise 2.2</h2>\n<p>For each value, write down the number of partitions, whether there is a partitioner assigned to and if so, which one. Also, briefly indicate how you got the answer.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142659_489131558",
      "id": "paragraph_1631202698099_1838796185",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:367"
    },
    {
      "text": "%spark\nval F = C.map( {case(x,y) => (x, y.reduce((a,b) => a + b))} )\nF.take(20)",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142659_1029799839",
      "id": "20210323-214017_546452301",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:368"
    },
    {
      "text": "%md\nF is obtained by applying map to C, which does not change the number of partitions or assign a partitioner to F, then the number of partitions of F is 8, and its assigned partitioner is groupByKey.",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T21:42:12+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679002846607_717545709",
      "id": "paragraph_1679002846607_717545709",
      "dateCreated": "2023-03-16T21:40:46+0000",
      "status": "READY",
      "$$hashKey": "object:369"
    },
    {
      "text": "%spark\nval G = C.mapValues( y => y.reduce((a,b) => a + b))\nG.take(10)",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142659_692865755",
      "id": "paragraph_1638908693622_1541707331",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:370"
    },
    {
      "text": "%md\nmapValues transformation is a narrow transformation that does not change the partitioning of an RDD. Therefore, the number of partitions in G will be the same as C. However, since mapValues only applies the function to the values of the RDD, the keys are preserved and the partitioner is also preserved. So the number of partitions of G is 8, and its assigned partitioner is HashPartitioner.\n",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T21:43:46+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679002932185_116791602",
      "id": "paragraph_1679002932185_116791602",
      "dateCreated": "2023-03-16T21:42:12+0000",
      "status": "READY",
      "$$hashKey": "object:371"
    },
    {
      "text": "%md\n## Exercise 2.3\na. Look up documentation of `repartition()` and `coalesce()` to find out what each of the functions does. Describe the differences. \nb. Take a look at the [Spark UI](http://localhost:4040) (after executing the cells below). Explain why the `takeSample()` of value I requires one shuffle phase less than H. \n\na. repartition(numPartitions: Int) and coalesce(numPartitions: Int, shuffle: Boolean = false) are both used to change the number of partitions of an RDD. The main difference between them is that repartition() always shuffles the data, while coalesce() only shuffles the data if shuffle is set to true\n\nb. The takeSample() of value I requires one shuffle phase less than value H because value I is created by calling coalesce() with 4 partitions and shuffle set to true, while value H is created by calling repartition() with 4 partitions. This means that value I had to perform a shuffle operation to evenly redistribute the data among the new partitions, while value H had already shuffled the data into 8 partitions before calling repartition() to reduce the number of partitions to 4. Therefore, value I needed to perform one shuffle phase to redistribute the data among the new partitions, while value H needed to perform two shuffle phases: one to shuffle the data into 4 partitions, and another one to shuffle the data again into 4 partitions.",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T21:45:25+0000",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Exercise 2.3</h2>\n<p>a. Look up documentation of <code>repartition()</code> and <code>coalesce()</code> to find out what each of the functions does. Describe the differences.<br />\nb. Take a look at the <a href=\"http://localhost:4040\">Spark UI</a> (after executing the cells below). Explain why the <code>takeSample()</code> of value I requires one shuffle phase less than H.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142659_1892066006",
      "id": "paragraph_1631197533979_69515493",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:372"
    },
    {
      "text": "%spark\nval H = G.repartition(4)\nH.takeSample(true, 10)",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142659_1868032436",
      "id": "20210323-214017_1632643197",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:373"
    },
    {
      "text": "%spark\nH.toDebugString",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142659_1192758118",
      "id": "paragraph_1631199440086_136732456",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:374"
    },
    {
      "text": "%spark\nval I = G.coalesce(4)\nI.takeSample(true, 10)",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142659_1652134142",
      "id": "20210323-214017_1102115685",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:375"
    },
    {
      "text": "%spark\nI.toDebugString",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142659_1913221597",
      "id": "paragraph_1631199441170_1752903184",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:376"
    },
    {
      "text": "%md\n## Exercise 3\nWe return to the Shakespeare data that we already saw in the reader.\na. Create `val lines`. _Hint: this is also done in the reader._",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Exercise 3</h2>\n<p>We return to the Shakespeare data that we already saw in the reader.<br />\na. Create <code>val lines</code>. <em>Hint: this is also done in the reader.</em></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142659_1116013063",
      "id": "paragraph_1631794513100_1892205778",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:377"
    },
    {
      "text": "%spark\nval lines = sc.textFile(\"file:///opt/hadoop/100.txt\")",
      "user": "anonymous",
      "dateUpdated": "2023-03-18T20:49:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mlines\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[String]\u001b[0m = file:///opt/hadoop/100.txt MapPartitionsRDD[3] at textFile at <console>:26\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142659_336218008",
      "id": "paragraph_1644578339988_247896699",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "dateStarted": "2023-03-18T20:49:51+0000",
      "dateFinished": "2023-03-18T20:49:51+0000",
      "status": "FINISHED",
      "$$hashKey": "object:378"
    },
    {
      "text": "%md\nb. Find the length of the longest sentence in the corpus using the Map Reduce pattern, knowing that in scala you can write `4 max 6` to get 6 as their maximum. _Hint: look at the map-reduce of counting chars in the reader._",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>b. Find the length of the longest sentence in the corpus using the Map Reduce pattern, knowing that in scala you can write <code>4 max 6</code> to get 6 as their maximum. <em>Hint: look at the map-reduce of counting chars in the reader.</em></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_1086650903",
      "id": "paragraph_1631794514388_1658014158",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:379"
    },
    {
      "text": "%spark\nval sentences = lines.flatMap(line => line.split(\"\\\\.\"))\nval sentenceLengths = sentences.map(sentence => (sentence, sentence.length))\n\n// Reduce: For each sentence, reduce the list of its lengths to a single maximum value.\nval maxLength = sentenceLengths.reduceByKey((length1, length2) => length1 max length2)\n  .map(_._2).max()\n\nprintln(\"The length of the longest sentence is: \" + maxLength)",
      "user": "anonymous",
      "dateUpdated": "2023-03-18T20:49:54+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "The length of the longest sentence is: 72\n\u001b[1m\u001b[34msentences\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[String]\u001b[0m = MapPartitionsRDD[4] at flatMap at <console>:26\n\u001b[1m\u001b[34msentenceLengths\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, Int)]\u001b[0m = MapPartitionsRDD[5] at map at <console>:27\n\u001b[1m\u001b[34mmaxLength\u001b[0m: \u001b[1m\u001b[32mInt\u001b[0m = 72\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://94a6d63f794b:4040/jobs/job?id=2",
              "$$hashKey": "object:4274"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_202341816",
      "id": "paragraph_1631794516065_1023514008",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "dateStarted": "2023-03-18T20:49:54+0000",
      "dateFinished": "2023-03-18T20:50:00+0000",
      "status": "FINISHED",
      "$$hashKey": "object:380"
    },
    {
      "text": "%md \nLet's run a word count on Shakespeare again by executing the cells below. ",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Let&rsquo;s run a word count on Shakespeare again by executing the cells below.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_779805655",
      "id": "paragraph_1631794516979_1634986002",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:381"
    },
    {
      "text": "%spark\nval words = lines.flatMap(line => line.split(\" \"))\n              .filter(_ != \"\")\n              .map(word => (word,1))\n              .reduceByKey(_ + _)",
      "user": "anonymous",
      "dateUpdated": "2023-03-18T21:14:33+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_321210864",
      "id": "paragraph_1638293827329_547083269",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "FINISHED",
      "$$hashKey": "object:382",
      "dateFinished": "2023-03-18T21:14:34+0000",
      "dateStarted": "2023-03-18T21:14:33+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mwords\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, Int)]\u001b[0m = ShuffledRDD[24] at reduceByKey at <console>:29\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nwords.take(10)",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_1680840604",
      "id": "paragraph_1639513911785_688468669",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:383"
    },
    {
      "text": "%spark\nwords.filter(_._1 == \"Julia\").collect\n  .map({case (w,c) => \"%s occurs %d times\".format(w,c)}).map(println)",
      "user": "anonymous",
      "dateUpdated": "2023-03-18T21:06:29+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://94a6d63f794b:4040/jobs/job?id=9",
              "$$hashKey": "object:4659"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_1668089576",
      "id": "paragraph_1631794611208_458640180",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "FINISHED",
      "$$hashKey": "object:384",
      "dateFinished": "2023-03-18T21:06:31+0000",
      "dateStarted": "2023-03-18T21:06:29+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Julia occurs 12 times\n\u001b[1m\u001b[34mres9\u001b[0m: \u001b[1m\u001b[32mArray[Unit]\u001b[0m = Array(())\n"
          }
        ]
      }
    },
    {
      "text": "%md\nIn the reader, the count for “Julia” was 145. \nc. Why are the counts different now, and give examples of words that are not count.\n\nc.The counts are different now because the following map is used in the reader file word count: .map(w => w.toLowerCase().replaceAll(\"(^[^a-z]+|[^a-z]+$)\", \"\")).\nWith this map, each word is converted to a lowercase word and replaceAll(\"(^[^a-z]+|[^a-z]+$)\", \"\") removes any non-alphabetic characters that occur at the beginning or end of each word.\nSo for example in the reader, the words \"Julia\" and \"JuliA\" are considered as the word \"julia\", which is not the case. Here, the words \"Julia.\" or \"julia\" are not counted.",
      "user": "anonymous",
      "dateUpdated": "2023-03-18T20:55:55+0000",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>In the reader, the count for “Julia” was 145.<br />\nc. Why are the counts different now, and give examples of words that are not count.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_614003441",
      "id": "paragraph_1639513427120_40116472",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:385"
    },
    {
      "text": "%md\nNow we want to store the result of `words` to a textfile in the filesystem. \nd. Search for the correct function in the [RDD programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html) and use `file:///opt/hadoop/wc` as location.",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Now we want to store the result of <code>words</code> to a textfile in the filesystem.<br />\nd. Search for the correct function in the <a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\">RDD programming guide</a> and use <code>file:///opt/hadoop/wc</code> as location.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_1499412168",
      "id": "paragraph_1631794640182_1479472893",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:386"
    },
    {
      "text": "%spark\nwords.saveAsTextFile(\"file:///opt/hadoop/wc\")\n",
      "user": "anonymous",
      "dateUpdated": "2023-03-18T21:13:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_803477874",
      "id": "paragraph_1631794642824_582481467",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "ERROR",
      "$$hashKey": "object:387",
      "dateFinished": "2023-03-18T21:13:52+0000",
      "dateStarted": "2023-03-18T21:13:51+0000",
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/opt/hadoop/wc already exists\n  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n  ... 44 elided\n"
          }
        ]
      }
    },
    {
      "text": "%md\nWe can use a simple shell command to look into the directory that has been created.",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>We can use a simple shell command to look into the directory that has been created.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_1359972593",
      "id": "paragraph_1631794654282_2074133671",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:388"
    },
    {
      "text": "%sh\nls -al /opt/hadoop/wc",
      "user": "anonymous",
      "dateUpdated": "2023-03-18T21:06:47+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_145176707",
      "id": "paragraph_1631794667893_1654848878",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "FINISHED",
      "$$hashKey": "object:389",
      "dateFinished": "2023-03-18T21:06:52+0000",
      "dateStarted": "2023-03-18T21:06:47+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "total 972\ndrwxr-xr-x 2 hadoop hadoop   4096 Mar 18 21:06 .\ndrwx------ 1 hadoop hadoop   4096 Mar 18 21:06 ..\n-rw-r--r-- 1 hadoop hadoop      8 Mar 18 21:06 ._SUCCESS.crc\n-rw-r--r-- 1 hadoop hadoop   3792 Mar 18 21:06 .part-00000.crc\n-rw-r--r-- 1 hadoop hadoop   3804 Mar 18 21:06 .part-00001.crc\n-rw-r--r-- 1 hadoop hadoop      0 Mar 18 21:06 _SUCCESS\n-rw-r--r-- 1 hadoop hadoop 484301 Mar 18 21:06 part-00000\n-rw-r--r-- 1 hadoop hadoop 485394 Mar 18 21:06 part-00001\n"
          }
        ]
      }
    },
    {
      "text": "%md\n\nHowever, we can also look at it directly from your terminal. Issue the command `docker exec -it hey-spark /bin/bash` in a terminal on the machine running the notebook container and navigate to the same folder as above.",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>However, we can also look at it directly from your terminal. Issue the command <code>docker exec -it hey-spark /bin/bash</code> in a terminal on the machine running the notebook container and navigate to the same folder as above.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_1849256574",
      "id": "paragraph_1639514412539_1659579788",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:390"
    },
    {
      "text": "%md\nf. Inspect the files. How many and why are there multiple result files?\n\nf. There are 2 result files (part-00000 and part-00001), because the number of partitions of words is 2. The number of result files corresponds to the number of partitions in the RDD at the time of saving.",
      "user": "anonymous",
      "dateUpdated": "2023-03-18T21:22:05+0000",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>f. Inspect the files. How many and why are there multiple result files?</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142660_1306889489",
      "id": "paragraph_1631794679101_712928786",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:391"
    },
    {
      "text": "%md\ng. (optional/advanced) Complete freedom! Do whatever you want with the Shakespeare data. Do some different word counts, other filters, etc. \n\nIf you want to store another textfile to the file system, make sure to use another name than `wc`. Files are not automatically overwritten. \nYou can also clean up and delete the old directory, for this you can use the command below (for another file: change file name accordingly).",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>g. (optional/advanced) Complete freedom! Do whatever you want with the Shakespeare data. Do some different word counts, other filters, etc.</p>\n<p>If you want to store another textfile to the file system, make sure to use another name than <code>wc</code>. Files are not automatically overwritten.<br />\nYou can also clean up and delete the old directory, for this you can use the command below (for another file: change file name accordingly).</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142661_166400284",
      "id": "paragraph_1631794707296_339287076",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:392"
    },
    {
      "text": "%sh\nrm -rf /opt/hadoop/wc",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142661_412414307",
      "id": "paragraph_1631794717982_1676534120",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:393"
    },
    {
      "text": "%md\n## Wrap up\n\nIf you have reached this point properly and understood what you observed, you have a solid understanding of Spark and its execution model. \n\nExport your Exercises notebook by clicking `Export this note (zpln)` in the toolbar (do not export it as IPython, because it can mess up the cells) and submit your Zeppelin Notebook to the assignment box on Brightspace. After that, you can do the quiz to check your answers. If you have any questions left after that, do not hesistate and come to the lab sessions to discuss your assignment (or ask your question online in the matrix room).",
      "user": "anonymous",
      "dateUpdated": "2023-03-16T20:55:42+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Wrap up</h2>\n<p>If you have reached this point properly and understood what you observed, you have a solid understanding of Spark and its execution model.</p>\n<p>Export your Exercises notebook by clicking <code>Export this note (zpln)</code> in the toolbar (do not export it as IPython, because it can mess up the cells) and submit your Zeppelin Notebook to the assignment box on Brightspace. After that, you can do the quiz to check your answers. If you have any questions left after that, do not hesistate and come to the lab sessions to discuss your assignment (or ask your question online in the matrix room).</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1679000142661_1800190952",
      "id": "20210323-214017_672918386",
      "dateCreated": "2023-03-16T20:55:42+0000",
      "status": "READY",
      "$$hashKey": "object:394"
    }
  ],
  "name": "A3_Exercises",
  "id": "2HV7EXBVR",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/A3_Exercises"
}